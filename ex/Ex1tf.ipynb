{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew Ng Coursera Machine Learning Course - Ex 1\n",
    "**Dean's TensorFlow Reimplementation Attempt**\n",
    "\n",
    "*9/3/2017*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dean's additional playing around with TensorFlow\n",
    "https://www.tensorflow.org/get_started/get_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.constant(3.0, dtype=tf.float32)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "3.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(C.eval(session=sess))\n",
    "    print(C.eval())\n",
    "    print(sess.run(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = tf.placeholder(dtype=tf.float32)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add_three_node = tf.add(P, C)  # Same as next line but not as compact\n",
    "add_three_node = P + C\n",
    "add_three_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "[ 5.  7.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(add_three_node.eval({P: 2.0}))\n",
    "    print(sess.run(add_three_node, {P: 2.0}))\n",
    "    print(sess.run(add_three_node, {'Placeholder:0': 2.0})) # Fails if name above diff\n",
    "    print(sess.run(add_three_node, {P: [2.0, 4.0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "S = tf.Session()\n",
    "print(S.run(add_three_node, {P: 2.0}))\n",
    "\n",
    "# Frees any resources like variables, \n",
    "# the with context manager above does this automatically.\n",
    "S.close() \n",
    "\n",
    "# print(S.run(add_three_node, {P: 2.0}))  # would error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'eye/MatrixDiag:0' shape=(5, 5) dtype=int8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.eye(5, dtype=tf.int8)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(A))\n",
    "    print(A.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression with one variable\n",
    "\n",
    "**Skip down to 3 (multivariate linear regression) in tensorflow reimplementation.**  \n",
    "\n",
    "Non-tensorflow implementation is here for reference.\n",
    "\n",
    "### 2.1. Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1/ex1data1.txt', delimiter=',')\n",
    "# print(type(data))\n",
    "# print(data.shape)\n",
    "# print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnp = data[:,0]\n",
    "ynp = data[:,1]\n",
    "# m = len(y)  # Number of training examples.\n",
    "# print(X[:5])\n",
    "# print(y[:5])\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Xnp, ynp, 'rx', markersize=5)\n",
    "plt.xlabel('Profit in $10,000s')\n",
    "plt.ylabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gradient Descent\n",
    "### 2.2.1. Update Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis (univariate):\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1$$\n",
    "\n",
    "Hypothesis (multivariate):\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n$$\n",
    "\n",
    "Hypothesis incorporating an $x_0$ (always 1) to make the bias term more consistent:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n  \\mid  x_0 = 1$$\n",
    "\n",
    "Hypothesis vectorized:\n",
    "\n",
    "$$h_\\theta(x) = \\theta^\\top x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Note:** Superscripts in parenthesis $^{(i)}$ just refers to the $i^{th}$ training example, not raising to a power.  Superscripts that are not in parenthesis, like the final square in the cost function $^2$, does raise to a power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch gradient descent procedure, repeatedly updating $\\theta_j$ for all $j$:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta) $$\n",
    "\n",
    "... which is ...\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $$\n",
    "\n",
    "... updating $\\theta_j$ for all $j$ **simultaneously**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make X and y two dimensional.\n",
    "#print(X.shape)\n",
    "#X = X[:, np.newaxis]\n",
    "#y = y[:, np.newaxis]\n",
    "#print(X.shape)\n",
    "#print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "#theta = np.zeros((2,1))\n",
    "#theta\n",
    "W = tf.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize gradient descent settings\n",
    "#iterations = 1500\n",
    "#alpha = 0.01 # Learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Computing the cost $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def computeCost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = X @ theta\n",
    "    J = np.sum((h - y)**2) / (2*m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta)\n",
    "print('Cost at theta 0, 0: %f' % J)\n",
    "print('Expected cost about 32.07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array(((-1,) \n",
    "                 ,( 2,)))\n",
    "J = computeCost(X, y, theta)\n",
    "print('Cost at theta -1, 2: %f' % J)\n",
    "print('Expected cost about 54.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X[:5])\n",
    "print(y[:5])\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = np.zeros(num_iters) \n",
    "    for n in range(num_iters):\n",
    "        h = X @ theta\n",
    "        gradient = (X.T @ (h - y)) / m\n",
    "        theta_new = theta - alpha * gradient\n",
    "        theta = theta_new\n",
    "        J_history[n] = computeCost(X, y, theta)\n",
    "    return theta, J_history  # J_history used in optional exercises far below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((2,1))\n",
    "trained_theta, _ = gradientDescent(X, y, initial_theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expected theta values (approx) -3.6303, 1.1664\n",
    "trained_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:,1], y, 'rx', markersize=5)\n",
    "plt.xlabel('Profit in $10,000s')\n",
    "plt.ylabel('Population of City in 10,000s')\n",
    "plt.plot(X[:,1], X@trained_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Visualizing $J(\\theta)$\n",
    "(skipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Exercises\n",
    "# 3. Linear regression with multiple variables\n",
    "**Note: Some of this not tested as well as the above**\n",
    "\n",
    "First get data for multivariate regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 2)\n",
      "(47,)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('ex1/ex1data2.txt', delimiter=',')\n",
    "Xnp = data[:, :-1]\n",
    "ynp = data[:, -1]\n",
    "m = Xnp.shape[0]\n",
    "print(Xnp.shape)\n",
    "print(ynp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.10400000e+03,   3.00000000e+00],\n",
       "       [  1.60000000e+03,   3.00000000e+00],\n",
       "       [  2.40000000e+03,   3.00000000e+00],\n",
       "       [  1.41600000e+03,   2.00000000e+00],\n",
       "       [  3.00000000e+03,   4.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 399900.,  329900.,  369000.,  232000.,  539900.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization (aka feature scaling?) is adjusting the input features to be similar in size.  It can help gradient descent to converge more quickly.\n",
    "\n",
    "For example, mean normalization:\n",
    "\n",
    "$$x_j^{(i)} := \\frac{x_j^{(i)} - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "...where $\\mu_j$ is the mean of all $x_j$, and $\\sigma_j$ is the standard deviation of all $x_j$\n",
    "\n",
    "**Note:  Remember to apply the same treatment to any input variables of new data you make predictions on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.10400000e+03,   3.00000000e+00],\n",
       "       [  1.60000000e+03,   3.00000000e+00],\n",
       "       [  2.40000000e+03,   3.00000000e+00],\n",
       "       [  1.41600000e+03,   2.00000000e+00],\n",
       "       [  3.00000000e+03,   4.00000000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements\n",
    "def featureNormalize(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = X.std(axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnp, mu, sigma = featureNormalize(Xnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13141542, -0.22609337],\n",
       "       [-0.5096407 , -0.22609337],\n",
       "       [ 0.5079087 , -0.22609337],\n",
       "       [-0.74367706, -1.5543919 ],\n",
       "       [ 1.27107075,  1.10220517]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepend ones for bias/intercept term\n",
    "#Xnp = np.hstack((np.ones((m, 1)), Xnp))\n",
    "#Xnp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.0. Make Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"y:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    X = tf.placeholder(tf.float32, name='X')\n",
    "    y = tf.placeholder(tf.float32, name='y')\n",
    "    feature_count = Xnp.shape[-1]\n",
    "    W = tf.Variable(tf.zeros([Xnp.shape[-1]], dtype=tf.float32))\n",
    "    b = tf.Variable(tf.zeros([], dtype=tf.float32))\n",
    "    print(y)\n",
    "    dummy = y**2\n",
    "    #w = tf.Variable(X.get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 399900.  329900.  369000.  232000.  539900.  299900.  314900.  198999.\n",
      "  212000.  242500.  239999.  347000.  329999.  699900.  259900.  449900.\n",
      "  299900.  199900.  499998.  599000.  252900.  255000.  242900.  259900.\n",
      "  573900.  249900.  464500.  469000.  475000.  299900.  349900.  169900.\n",
      "  314900.  579900.  285900.  249900.  229900.  345000.  549000.  287000.\n",
      "  368500.  329900.  314000.  299000.  179900.  299900.  239500.]\n",
      "[ 0.  0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inputs = {X: Xnp, y: ynp}\n",
    "    print(y.eval(feed_dict=inputs))\n",
    "    print(sess.run(W, feed_dict=inputs))\n",
    "    print(sess.run(b, feed_dict=inputs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before exception\n",
      "In Exception test\n",
      "In finally\n"
     ]
    }
   ],
   "source": [
    "# I tried splitting these into multiple cells, and it didn't seem to work.\n",
    "# So I guess no multicell try finally's to close tf.Session's, etc.\n",
    "try:\n",
    "    print('Before exception')\n",
    "    raise Exception('test')\n",
    "    print('After exception')\n",
    "except Exception as E:\n",
    "    print('In Exception %s' % E)\n",
    "finally:\n",
    "    print('In finally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gradient Descent - Choosing Learning Rate $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want a learning rate small enough to converge, but not so small that it takes forever to converge.  It can be helpful to graph cost $J(\\theta)$ for each iteration of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "num_iters = 400;\n",
    "theta = np.zeros(X.shape[-1])\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained, J_history = gradientDescent(X, y, theta, alpha, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Optional (ungraded) exercise: Selecting learning rates\n",
    "\n",
    "Try out different learning rates $\\alpha$, (e.g. 0.3, 0.1, 0.03, 0.01), looking for a rate that converges quickly.  Plot the cost function $J(\\theta)$ convergence over the first 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "plt.plot(range(len(J_history)), J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.03\n",
    "num_iters = 50\n",
    "theta_trained, J_history = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "plt.yscale('log')\n",
    "plt.plot(range(len(J_history)), J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "num_iters = 50\n",
    "theta_trained, J_history = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "plt.yscale('log')\n",
    "plt.plot(range(len(J_history)), J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained value of $\\theta$ to predict the price of a house with 1650 square feet and 3 bedrooms.  (Don't forget to normalize your features when you make this prediction.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xpred = np.array([1650, 3])\n",
    "Xpred = (Xpred - mu) / sigma\n",
    "Xpred = np.hstack(([1], Xpred))\n",
    "Xpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = theta_trained @ Xpred  # Should one of these be transposed?\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is applicable to many other situations.  However, for linear regression, there is a way to solve for the optimal parameters directly, without feature scaling or iterating.  However, this normal equation can be slow if the number of features $n$ is very large, e.g $n > 10,000$.  It scales approximately $O(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = (X^\\top X)^{-1} X^\\top \\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use a (`np.linalg.pinv()`?) pseudo-inverse function instead of a normal inverse, just in case $X^TX$ is non-invertable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    theta = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained = normalEqn(X, y)\n",
    "theta_trained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
