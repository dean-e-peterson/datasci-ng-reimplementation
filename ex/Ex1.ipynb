{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew Ng Coursera Machine Learning Course - Ex 1\n",
    "**Dean's Reimplementation Attempt**\n",
    "\n",
    "*9/4/2017*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.eye(5)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression with one variable\n",
    "### 2.1. Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1/ex1data1.txt', delimiter=',')\n",
    "# print(type(data))\n",
    "# print(data.shape)\n",
    "# print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:,0]\n",
    "y = data[:,1]\n",
    "m = len(y)  # Number of training examples.\n",
    "# print(X[:5])\n",
    "# print(y[:5])\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'rx', markersize=5)\n",
    "plt.xlabel('Profit in $10,000s')\n",
    "plt.ylabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gradient Descent\n",
    "### 2.2.1. Update Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis (univariate):\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1$$\n",
    "\n",
    "Hypothesis (multivariate):\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n$$\n",
    "\n",
    "Hypothesis incorporating an $x_0$ (always 1) to make the bias term more consistent:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n  \\mid  x_0 = 1$$\n",
    "\n",
    "Hypothesis vectorized:\n",
    "\n",
    "$$h_\\theta(x) = \\theta^Tx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Note:** Superscripts in parenthesis $^{(i)}$ just refers to the $i^{th}$ training example, not raising to a power.  Superscripts that are not in parenthesis, like the final square in the cost function $^2$, does raise to a power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch gradient descent procedure, repeatedly updating $\\theta_j$ for all $j$:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta) $$\n",
    "\n",
    "... which is ...\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $$\n",
    "\n",
    "... updating $\\theta_j$ for all $j$ **simultaneously**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Implementation\n",
    "\n",
    "Prepend a column of ones to X, for the bias term, the $x_0$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make X and y two dimensional.\n",
    "print(X.shape)\n",
    "X = X[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "print(X.shape)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepend ones\n",
    "X = np.hstack((np.ones((m,1)), X))\n",
    "print(X.shape)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "theta = np.zeros((2,1))\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01 # Learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Computing the cost $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def computeCost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = X @ theta\n",
    "    J = np.sum((h - y)**2) / (2*m)\n",
    "    # Could have also squared by calculating diff=h-y, then diff.T times diff.\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta)\n",
    "print('Cost at theta 0, 0: %f' % J)\n",
    "print('Expected cost about 32.07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array(((-1,) \n",
    "                 ,( 2,)))\n",
    "J = computeCost(X, y, theta)\n",
    "print('Cost at theta -1, 2: %f' % J)\n",
    "print('Expected cost about 54.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X[:5])\n",
    "print(y[:5])\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = np.zeros(num_iters) \n",
    "    for n in range(num_iters):\n",
    "        h = X @ theta\n",
    "        gradient = (X.T @ (h - y)) / m\n",
    "        theta_new = theta - alpha * gradient\n",
    "        theta = theta_new\n",
    "        J_history[n] = computeCost(X, y, theta)\n",
    "    return theta, J_history  # J_history used in optional exercises far below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((2,1))\n",
    "trained_theta, _ = gradientDescent(X, y, initial_theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expected theta values (approx) -3.6303, 1.1664\n",
    "trained_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:,1], y, 'rx', markersize=5)\n",
    "plt.xlabel('Profit in $10,000s')\n",
    "plt.ylabel('Population of City in 10,000s')\n",
    "plt.plot(X[:,1], X@trained_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Visualizing $J(\\theta)$\n",
    "(skipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Exercises\n",
    "# 3. Linear regression with multiple variables\n",
    "**Note: Some of this not tested as well as the above**\n",
    "\n",
    "First get data for multivariate regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1/ex1data2.txt', delimiter=',')\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "m = X.shape[0]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization (aka feature scaling?) is adjusting the input features to be similar in size.  It can help gradient descent to converge more quickly.\n",
    "\n",
    "For example, mean normalization:\n",
    "\n",
    "$$x_j^{(i)} := \\frac{x_j^{(i)} - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "...where $\\mu_j$ is the mean of all $x_j$, and $\\sigma_j$ is the standard deviation of all $x_j$\n",
    "\n",
    "**Note:  Remember to apply the same treatment to any input variables of new data you make predictions on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements\n",
    "def featureNormalize(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = X.std(axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, mu, sigma = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepend ones for bias/intercept term\n",
    "X = np.hstack((np.ones((m, 1)), X))\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gradient Descent - Choosing Learning Rate $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want a learning rate small enough to converge, but not so small that it takes forever to converge.  It can be helpful to graph cost $J(\\theta)$ for each iteration of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "num_iters = 400;\n",
    "theta = np.zeros(X.shape[-1])\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained, J_history = gradientDescent(X, y, theta, alpha, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Optional (ungraded) exercise: Selecting learning rates\n",
    "\n",
    "Try out different learning rates $\\alpha$, (e.g. 0.3, 0.1, 0.03, 0.01), looking for a rate that converges quickly.  Plot the cost function $J(\\theta)$ convergence over the first 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "plt.plot(range(len(J_history)), J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.03\n",
    "num_iters = 50\n",
    "theta_trained, J_history = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "plt.yscale('log')\n",
    "plt.plot(range(len(J_history)), J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "num_iters = 50\n",
    "theta_trained, J_history = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "plt.yscale('log')\n",
    "plt.plot(range(len(J_history)), J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained value of $\\theta$ to predict the price of a house with 1650 square feet and 3 bedrooms.  (Don't forget to normalize your features when you make this prediction.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xpred = np.array([1650, 3])\n",
    "Xpred = (Xpred - mu) / sigma\n",
    "Xpred = np.hstack(([1], Xpred))\n",
    "Xpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = theta_trained @ Xpred  # Should one of these be transposed?\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is applicable to many other situations.  However, for linear regression, there is a way to solve for the optimal parameters directly, without feature scaling or iterating.  However, this normal equation can be slow if the number of features $n$ is very large, e.g $n > 10,000$.  It scales approximately $O(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = (X^T X)^{-1} X^T \\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use a (`np.linalg.pinv()`?) pseudo-inverse function instead of a normal inverse, just in case $X^TX$ is non-invertable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    theta = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
