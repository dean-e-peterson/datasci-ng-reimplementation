{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew Ng Coursera Machine Learning Course - Ex 2\n",
    "**Dean's Reimplementation Attempt**\n",
    "\n",
    "*9/23/2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: See [here](https://docs.scipy.org/doc/scipy/reference/api.html#guidelines-for-importing-functions-from-scipy) for recommendations on how to import `scipy` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "**Note:** This is a classification algorithm, despite the term \"regression\" in the name.  Classification means outputs $y$ take on discrete values, for example $y \\in \\{0,1\\}$.  The output is  also known as a *label*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sigmoid function*, aka *logistic function*\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-4, 4, 81)\n",
    "g = 1 / (1 + np.exp(-z))\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.plot(z, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hypothesis, use:\n",
    "\n",
    "$$z = \\theta^Tx$$\n",
    "\n",
    "$$h_\\theta(x) = g(z) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}} $$\n",
    "\n",
    "... making $h_\\theta(x)$ the probability that the output is 1, not 0.\n",
    "\n",
    "$$h_\\theta(x) = P(y=1 \\: | \\: x;\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decision boundary, if $h_\\theta(x) \\ge 0.5$, predict $y = 1$.  This happens when $z \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cost function, use:\n",
    "\n",
    "$$\\text{Cost}(h_\\theta(x), y) = - \\log_e(h_\\theta(x)) \\qquad\\qquad \\text{if} \\, y=1 $$\n",
    "$$\\text{Cost}(h_\\theta(x), y) = - \\log_e(1 - h_\\theta(x)) \\qquad \\text{if} \\, y=0 $$\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\text{Cost}\\left(h_\\theta(x^{(i)}, y^{(i)}\\right) $$\n",
    "\n",
    "...so...\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ \n",
    "y^{(i)} \\log(h_\\theta(x^{(i)}))\n",
    "+ (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))\n",
    "\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized, this is:\n",
    "$$J(\\theta) = \\frac{1}{m} \\left( -y^\\top\\log(h) - (1-y)^\\top\\log(1-h) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is:\n",
    "$$\\frac{\\partial}{\\partial\\theta_j} = \\frac{1}{m} \\sum_{i=1}^m\n",
    "  \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}$$\n",
    "\n",
    "So the gradient descent rule is:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m\n",
    "  \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}$$\n",
    "...which *looks* the same as for linear regression, but remember that $h_\\theta(x^{(i)})$ is the sigmoid function this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Visualizing the data\n",
    "\n",
    "See [numpy.matmul() documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html) for how a 1-D numpy array is treated by matrix multiplication.  \n",
    "\n",
    "Basically, given vector v and matrix M, in **`v@M`** the v is treated as a row vector, but in **`M@v`** the v is treated as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex2/ex2data1.txt', delimiter=',')\n",
    "X = data[:, 0:2]\n",
    "y = data[:, 2]    # y = data[:, 2:3] for 1-column 2-d array.\n",
    "print(data.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(data[:5])\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = np.where(y == 1)\n",
    "neg = np.where(y == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lists of \"lines\" created (l1...) for the legend below.\n",
    "l1 = plt.plot(X[pos, 0], X[pos, 1], 'k+', label='Admitted')\n",
    "l2 = plt.plot(X[neg, 0], X[neg, 1], 'yo', label='Not admitted')\n",
    "\n",
    "# plt.legend() puts one legend entry for each data point,\n",
    "# so pick just the first point from each class and specify those.\n",
    "plt.legend((l1[0], l2[0]), (l1[0].get_label(), l2[0].get_label()))\n",
    "\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the intercept/bias term of all 1's to X.\n",
    "m = X.shape[0]\n",
    "ones = np.ones((m, 1))\n",
    "X = np.hstack((ones, X))\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1. Warmup exercise: sigmoid function\n",
    "*Sigmoid function*, aka *logistic function*\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def sigmoid(X):\n",
    "    g = 1 / (1 + np.exp(-X))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2. Cost function and gradient\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ \n",
    "y^{(i)} \\log(h_\\theta(x^{(i)}))\n",
    "+ (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))\n",
    "\\right] $$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j} = \\frac{1}{m} \\sum_{i=1}^m\n",
    "  \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(X.shape[1]) # np.zeros((X.shape[1], 1)), for n x 1 2d array\n",
    "initial_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def costFunction(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(X @ theta) # Not theta.T @ X because I wanted samples in rows?\n",
    "\n",
    "    Jsumofones = y.T @ np.log(h) \n",
    "    Jsumofzeros = (1-y).T @ np.log(1-h)\n",
    "    J = (-1/m) * (Jsumofones + Jsumofzeros)\n",
    "    \n",
    "    grad = (1/m) * X.T @ (h - y)\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost, grad = costFunction(initial_theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected 0.693, and [-0.1, -12.0092, -11.2628]\n",
    "print(cost) # 1-d vector input made this a scalar, not a 2-d array.\n",
    "print(grad) # 1-d vector input etc made this a 1-d array, not a 2-d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra parentheses and commas would make the following a column vector.\n",
    "#test_theta = np.array(((-24,), (0.2,), (0.2,)))\n",
    "test_theta = np.array((-24, 0.2, 0.2))\n",
    "print(test_theta.shape)\n",
    "cost, grad = costFunction(test_theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected cost (approx): 0.218\n",
    "# Expected gradients (approx):  0.043, 2.566, 2.647');\n",
    "print(cost) # Should this be coming out as a scalar, not a 2-d array?\n",
    "print(grad) # Should this be coming out as a 1-d array, not a 2-d array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2.3. Learning parameters using `scipy.optimize.minimize`\n",
    "`scipy.optimize.minimize()` calls back to the objective function with only the parameter vector being optimized (in our case, `theta`).  So we need to make a partial function that only accepts `theta` and always uses X and y built-in.  We can do this as separate function definition or as a lambda.  So first we make basically a partial function based on our `costFunction()` that will still accept `theta`, but will fill in `X` and `y` for us.\n",
    "\n",
    "Note: This has to do with partial as a functional programming concept, not partial derivatives.\n",
    "\n",
    "Note2: It would have been nice to do this with the `partial()` function in the `functools` module, but I don't think the original order of parameters allows that.\n",
    "\n",
    "Note3: Whichever way we pick, this is the same thing as Andrew Ng's original does in Octave/Matlab with the `@(t) (costFunction(t, X, y))` syntax. `optimize.minimize(lambda t: costFunction(t, X, y), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The non-lambda way to make a cost function to pass to optimize.minimize()\n",
    "#def costFun(theta):\n",
    "#    J, grad = costFunction(theta, X, y)\n",
    "#    return J, grad\n",
    "#    # return J[0], grad[:,0] # This was if using 2-d theta input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros([X.shape[1]])\n",
    "initial_theta = test_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  Was using `fminunc()` in Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = optimize.minimize(\n",
    "    lambda t: costFunction(t, X, y), \n",
    "    initial_theta, \n",
    "    jac=True, \n",
    "    options={'maxiter':100, 'disp':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print theta to screen\n",
    "print('Cost at theta found by optimize: %s' % result.fun)\n",
    "print('Expected cost (approx):          0.203')\n",
    "print('theta:                           %s' % result.x)\n",
    "print('Expected theta (approx):         -25.161, 0.206, 0.201')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2.4. Evaluating Logistic Regression\n",
    "TODO: Graph decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try predicting one student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = result.x\n",
    "#X_one = np.array(((1, 45, 85), ))\n",
    "X_one = np.array((1, 45, 85))\n",
    "h_one = sigmoid(X_one @ theta)\n",
    "h_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm.\n",
    "print('For scores 45 and 85, we predict admission probability %f' % h_one)\n",
    "print('Expected value: 0.775 +/- 0.002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check overall accuracy on the data we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student implements...\n",
    "def predict(theta, X):\n",
    "    probabilities = sigmoid(X @ theta)\n",
    "    return (probabilities >= 0.5).astype(np.float)\n",
    "#predict(theta, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predict(theta, X)\n",
    "accuracy = (p == y).astype(np.float).mean()\n",
    "accuracy\n",
    "\n",
    "print('Train Accuracy: %f%%', accuracy * 100);\n",
    "print('Expected accuracy (approx): 89.0');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex2/ex2data2.txt', delimiter=',')\n",
    "X = data[:, 0:-1]\n",
    "y = data[:, -1]\n",
    "print(data[:5])\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = np.where(y == 1)\n",
    "neg = np.where(y == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Why does this plot look different than Ex2.pdf handout? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = plt.plot(X[pos, 0], X[pos, 1], 'k+', label='Passed')\n",
    "l2 = plt.plot(X[neg, 0], X[neg, 1], 'yo', label='Failed')\n",
    "plt.legend((l1[0], l2[0]), (l1[0].get_label(), l2[0].get_label()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Feature mapping\n",
    "\n",
    "In addition to adding the bias term of 1's, we map the 2 features into a bunch of features that include higher order polynomials of those two features.  This lets us fit more complex functions, although it is more prone to overfitting as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Add leading 1's column for intercept/bias term, and add \n",
    "# polynomial terms, but only for two starting features x1 & x2.\n",
    "def mapFeature(x1, x2, degree=6, bias=True):\n",
    "\n",
    "    # Calculate total number of terms. (Is there a more general way?)\n",
    "    cols = degree * (degree + 3) // 2\n",
    "    if bias:\n",
    "        cols += 1\n",
    "\n",
    "    # Allocate uninitialized array\n",
    "    out = np.empty((len(x1), cols))\n",
    "    nextcol = 0\n",
    "    \n",
    "    # Add bias term if requested\n",
    "    if bias:\n",
    "        out[:, nextcol] = 1\n",
    "        nextcol += 1\n",
    "\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(0, i + 1):\n",
    "            out[:, nextcol] = (x1 ** (i-j)) * (x2 ** j)\n",
    "            nextcol += 1\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X[:2])\n",
    "X = mapFeature(X[:,0], X[:,1])\n",
    "print(X.shape)\n",
    "print(X[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters and lambda\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "lamb = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student implements cost function with regularzation\n",
    "def costFunctionReg(initial_theta, X, y, lamb):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost, grad = costFunctionReg(initial_theta, X, y, lamb)\n",
    "print('Cost at initial theta (zeros): ', cost)\n",
    "print('Expected cost (approx): 0.693')\n",
    "print('Gradient at initial theta (zeros) - first five values only:')\n",
    "print(grad[:5])\n",
    "print('Expected gradients (approx) - first five values only:')\n",
    "print('0.0085, 0.0188, 0.0001, 0.0503, 0.0115')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
